{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis von Tweets\n",
    "\n",
    "Das Projekt wurde im Rahmen der Veranstaltung zur maschinellen Sprachverarbeitung (Natural Language Processing, NLP) im WS 2021/22 an der Universität Tübingen von mir erfolgreich abgeschlossen. Die Hauptaufgabe des Projekts bestand darin, einen Klassifikator zu entwickeln, der entscheiden kann, ob eine Tweet positiv, negativ oder neutral ist. Als Daten für die von mir trainierten Klassifikatoren (Maximum Entropy Klassifikator, Support Vector Klassifikator und Decision Tree Klassifikator) dienten die Daten des SemEval 2013 Task 2b, ein Korpus mit 14882 annotierten Tweets. Informationen zum Korpus: https://aclanthology.org/S13-2052/. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die im Projekt verwendeten Methoden und Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Einlesen und Visualisierung von Daten\n",
    "import pandas as pd                          # Bibliothek zum Einlesen und Bearbeiten von Daten\n",
    "import matplotlib.pyplot as plt              # Bibliothek zur Visualisierung von Daten\n",
    "import seaborn as sns                        # Bibliothek zur Visualisierung von Daten\n",
    "\n",
    "# Normalisierung von Daten\n",
    "import re                                    # Modul zum Matchen von regulären Ausdrücken\n",
    "import nltk                                  # Bibliothek für Anwendungen der Computerlinguistik\n",
    "from nltk.tokenize import word_tokenize      # Methode zur Tokenisierung von Texten\n",
    "from nltk.corpus import stopwords            # Liste von englischen Stoppwörtern\n",
    "from nltk import FreqDist                    # Klasse zur Berechnung der Häufigkeiten von Wörtern\n",
    "from nltk.stem import PorterStemmer          # Klasse zum Stemming von Wörtern\n",
    "stop_words = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Klassifizierung\n",
    "from nltk import NaiveBayesClassifier                   # Naive Bayes Klassifikator\n",
    "from nltk.classify import DecisionTreeClassifier        # Decision Tree Klassifikator\n",
    "from nltk.classify import MaxentClassifier              # Maximum Entropy Klassifikator\n",
    "from nltk.classify.scikitlearn import SklearnClassifier # Sklearn Klassifikator (ermöglicht innerhalb von NLTK unterschiedliche Klassifikatoren zu nutzen)\n",
    "from sklearn.svm import SVC                             # Support Vector Klassifikator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1: Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9679</th>\n",
       "      <td>103158179306807296</td>\n",
       "      <td>positive</td>\n",
       "      <td>RT @MNFootNg It's monday and Monday Night Foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>103157324096618497</td>\n",
       "      <td>positive</td>\n",
       "      <td>All I know is the road for that Lomardi start ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9681</th>\n",
       "      <td>100259220338905089</td>\n",
       "      <td>neutral</td>\n",
       "      <td>All Blue and White fam, we r meeting at Golden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9682</th>\n",
       "      <td>104230318525001729</td>\n",
       "      <td>positive</td>\n",
       "      <td>@DariusButler28   Have a great game agaist Tam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9683</th>\n",
       "      <td>100461938533863424</td>\n",
       "      <td>negative</td>\n",
       "      <td>I'm pisseeedddd that I missed Kid Cudi's show ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id sentiment  \\\n",
       "9679  103158179306807296  positive   \n",
       "9680  103157324096618497  positive   \n",
       "9681  100259220338905089   neutral   \n",
       "9682  104230318525001729  positive   \n",
       "9683  100461938533863424  negative   \n",
       "\n",
       "                                                  tweet  \n",
       "9679  RT @MNFootNg It's monday and Monday Night Foot...  \n",
       "9680  All I know is the road for that Lomardi start ...  \n",
       "9681  All Blue and White fam, we r meeting at Golden...  \n",
       "9682  @DariusButler28   Have a great game agaist Tam...  \n",
       "9683  I'm pisseeedddd that I missed Kid Cudi's show ...  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drei Datensätze einlesen: Trainingsdaten (train_data), Entwicklungsdaten (dev_data) und Testdaten (test_data)\n",
    "train_data = pd.read_csv(\"semeval2013/twitter-2013train-A.txt\", sep = '\\t', header = None)\n",
    "test_data = pd.read_csv(\"semeval2013/twitter-2013test-A.txt\", sep = '\\t', header = None)\n",
    "dev_data = pd.read_csv(\"semeval2013/twitter-2013dev-A.txt\", sep = '\\t', header = None)\n",
    "\n",
    "# Spalten von jedem der drei Datensätze benennen: id, sentiment (Stimmungswert: positiv, neutral negativ), tweet\n",
    "train_data.columns = ['id', 'sentiment', 'tweet']\n",
    "test_data.columns = ['id', 'sentiment', 'tweet']\n",
    "dev_data.columns = ['id', 'sentiment', 'tweet']\n",
    "\n",
    "# Trainingsdaten enthalten 9683 Tweets = 65% vom ganzen Datensatz\n",
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3542</th>\n",
       "      <td>264233437060288512</td>\n",
       "      <td>negative</td>\n",
       "      <td>Khaleda Zia's present India visit may have a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3543</th>\n",
       "      <td>263779058284904448</td>\n",
       "      <td>neutral</td>\n",
       "      <td>FYI, golf fans: @jameslepp will join Moj on We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544</th>\n",
       "      <td>250744453831213056</td>\n",
       "      <td>negative</td>\n",
       "      <td>@__Aniko you think mr.Calle let practice with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545</th>\n",
       "      <td>258679974591090688</td>\n",
       "      <td>positive</td>\n",
       "      <td>Don't hide under your desk! It's just a salsa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3546</th>\n",
       "      <td>243018372453916674</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Saturday flashmob on the music of The Runaways...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id sentiment  \\\n",
       "3542  264233437060288512  negative   \n",
       "3543  263779058284904448   neutral   \n",
       "3544  250744453831213056  negative   \n",
       "3545  258679974591090688  positive   \n",
       "3546  243018372453916674   neutral   \n",
       "\n",
       "                                                  tweet  \n",
       "3542  Khaleda Zia's present India visit may have a b...  \n",
       "3543  FYI, golf fans: @jameslepp will join Moj on We...  \n",
       "3544  @__Aniko you think mr.Calle let practice with ...  \n",
       "3545  Don't hide under your desk! It's just a salsa ...  \n",
       "3546  Saturday flashmob on the music of The Runaways...  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testdaten enthalten 3546 Tweets = 24 % vom ganzen Datensatz\n",
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>264241571908681728</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#WEB YouTube improves upload process with opti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>264228980444495875</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gonna change my Tumblr theme. I hope I can fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>264210367192915968</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I\\u2019m so jealous of everyone at the Justin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>263737249240342528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Jim Harbaugh\\u002c Alex Smith Drive Giants Wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>264096818831433728</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#Trending: Tim Tebow is now dating cave woman ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id sentiment  \\\n",
       "1649  264241571908681728   neutral   \n",
       "1650  264228980444495875  positive   \n",
       "1651  264210367192915968   neutral   \n",
       "1652  263737249240342528   neutral   \n",
       "1653  264096818831433728   neutral   \n",
       "\n",
       "                                                  tweet  \n",
       "1649  #WEB YouTube improves upload process with opti...  \n",
       "1650  Gonna change my Tumblr theme. I hope I can fin...  \n",
       "1651  I\\u2019m so jealous of everyone at the Justin ...  \n",
       "1652  Jim Harbaugh\\u002c Alex Smith Drive Giants Wor...  \n",
       "1653  #Trending: Tim Tebow is now dating cave woman ...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entwicklungsdaten enthalten 1653 Tweets = 11 % vom ganzen Datensatz\n",
    "dev_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# die Distribution der Stimmungswerte der Trainigsdaten visualisieren\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 8))\n",
    "axs[0].set_title('Training Data')\n",
    "axs[1].set_title('Test Data')\n",
    "axs[2].set_title('Development Data')\n",
    "order_list = ['neutral', 'positive', 'negative']\n",
    "sns.histplot(train_data['sentiment'], color='cornflowerblue', alpha=1, ax=axs[0])\n",
    "sns.histplot(test_data['sentiment'], color='cornflowerblue', alpha=1, ax=axs[1])\n",
    "sns.histplot(dev_data['sentiment'], color='cornflowerblue', alpha=1, ax=axs[2])\n",
    "plt.show()\n",
    "# Man sieht, dass die Stimmungswerte in allen drei Datensätzen unterschiedliche Distributionen haben,\n",
    "# d.h. die Daten sind nicht balanciert:\n",
    "# Es gibt wenige negative Stimmungswerte und viele neutrale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2: Eine Baseline implementieren\n",
    "#### A2. Schritt 1: Textdaten (Tweets) normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(tweet):\n",
    "    \n",
    "    normalized_tweet = tweet.lower()\n",
    "    normalized_tweet = re.sub(r'https?:\\/\\/\\S+', '', normalized_tweet)\n",
    "    normalized_tweet = re.sub(r'@[A-Za-z0-9]+', '', normalized_tweet)\n",
    "    normalized_tweet = re.sub(r'#', '', normalized_tweet)\n",
    "    normalized_tweet = re.sub(r'\\\\u[a-z|0-9]{4}', '', normalized_tweet)\n",
    "    normalized_tweet = [token for token in word_tokenize(normalized_tweet) if token.isalpha() and not token in stop_words and len(token)>=3]\n",
    "     \n",
    "    return normalized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Normalisierungsfunktion auf Rohdaten (Tweets) anwenden\n",
    "train_data['normalized_tweet'] = train_data['tweet'].apply(normalize_data)\n",
    "test_data['normalized_tweet'] = test_data['tweet'].apply(normalize_data)\n",
    "dev_data['normalized_tweet'] = dev_data['tweet'].apply(normalize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drei Datensätze mit den normalisierten Tweets und den entsprechenden Stimmungswerten erstellen.\n",
    "# Diese werden später für Klassifiezierungsmodelle benötigt.\n",
    "tweets_train = list(zip(train_data['normalized_tweet'], train_data['sentiment']))    \n",
    "tweets_test = list(zip(test_data['normalized_tweet'], test_data['sentiment']))\n",
    "tweets_dev = list(zip(dev_data['normalized_tweet'], dev_data['sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2. Schritt 2: Tokens aus den normalisierten Tweets extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153237\n",
      "['gas', 'house', 'hit', 'going', 'chapel', 'hill', 'sat', 'theo', 'walcott', 'still']\n"
     ]
    }
   ],
   "source": [
    "def get_all_words_from_tweets(tweets):\n",
    "    \n",
    "    all_words = []\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            all_words.append(word)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "all_words_train = get_all_words_from_tweets(train_data['normalized_tweet'])\n",
    "all_words_test = get_all_words_from_tweets(test_data['normalized_tweet'])\n",
    "all_words_dev = get_all_words_from_tweets(dev_data['normalized_tweet'])\n",
    "all_words = all_words_train + all_words_test + all_words_dev\n",
    "\n",
    "print(len(all_words))\n",
    "print(all_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2. Schritt 3: Features (die 1000 häufigsten Tokens) extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tomorrow', 'may', 'day', 'night', 'going', 'see', 'saturday', 'sunday', 'game', 'time']\n"
     ]
    }
   ],
   "source": [
    "def get_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    most_common_features = wordlist.most_common(1000)\n",
    "    return [word for (word, freq) in most_common_features]\n",
    "\n",
    "word_features = get_features(all_words)\n",
    "print(word_features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2. Schritt 4: Das Vorhandensein von Features in den normalisierten Tweets feststellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[f'contains {word}'] = (word in set(tweet))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Vorhandensein von Features in den normalisierten Tweets feststellen\n",
    "labeled_features_train = [(extract_features(tweet), sentiment) for (tweet, sentiment) in tweets_train]\n",
    "labeled_features_test = [(extract_features(tweet), sentiment) for (tweet, sentiment) in tweets_test]\n",
    "labeled_features_dev = [(extract_features(tweet), sentiment) for (tweet, sentiment) in tweets_dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2. Schritt 5: Naive Bayes Classifier trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = NaiveBayesClassifier.train(labeled_features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3: Evaluierung durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes Classifier:  0.639413588948407\n",
      "Most Informative Features\n",
      "           contains fuck = True           negati : neutra =     91.2 : 1.0\n",
      "        contains amazing = True           positi : neutra =     49.1 : 1.0\n",
      "        contains excited = True           positi : neutra =     43.0 : 1.0\n",
      "           contains luck = True           positi : neutra =     39.9 : 1.0\n",
      "          contains thank = True           positi : neutra =     34.0 : 1.0\n",
      "          contains sorry = True           negati : neutra =     29.6 : 1.0\n",
      "          contains happy = True           positi : neutra =     29.5 : 1.0\n",
      "         contains injury = True           negati : positi =     27.5 : 1.0\n",
      "        contains awesome = True           positi : neutra =     27.0 : 1.0\n",
      "          contains great = True           positi : neutra =     23.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.accuracy(nb_classifier, labeled_features_test)\n",
    "print(\"Accuracy of Naive Bayes Classifier: \",accuracy)\n",
    "nb_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4 und A5: Ein weiteres Verfahren implementieren und solche Features wie Stamm/Lemma und POS-Tag ehtrahieren\n",
    "#### A4 und A5. Schritt 1: Normalisierung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rohdaten (Tweets) normalisieren. Diesmal werden die Tokens lemmatisiert.\n",
    "# Darüber hinaus wird jedem der Tokens ein POS-Tag zugeschrieben\n",
    "\n",
    "def normalize_data_2(tweet):\n",
    "    \n",
    "    # normalized_tweet = tweet.lower()\n",
    "    normalized_tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
    "    normalized_tweet = re.sub(r'@[A-Za-z0-9]+', '', normalized_tweet)\n",
    "    normalized_tweet = re.sub(r'#', '', normalized_tweet)\n",
    "    normalized_tweet = re.sub(r'\\\\u[a-z|0-9]{4}', '', normalized_tweet)\n",
    "    normalized_tweet = [token for token in word_tokenize(normalized_tweet) if token.isalpha() and not token in stop_words and len(token)>=3]\n",
    "    normalized_tweet = nltk.pos_tag(normalized_tweet)\n",
    "    normalized_tweet = [(ps.stem(word).lower(), tag) for word, tag in normalized_tweet]\n",
    "    \n",
    "    return normalized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Entfernen der POS-Tags. Diese wird später für die Anpassung von Trainings- und Testdaten benötigt.\n",
    "def remove_tags(normalized_tweet):\n",
    "    new_tweet = [word for (word, tag) in normalized_tweet]\n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(ga, NNP), (hous, NN), (hit, VBD), (go, VBG),...\n",
       "1    [(theo, NNP), (walcott, NNP), (still, RB), (sh...\n",
       "2    [(gsp, NNP), (fan, NN), (hate, NN), (nick, NNP...\n",
       "3    [(iranian, JJ), (gener, JJ), (say, VBZ), (isra...\n",
       "4    [(tehran, NNP), (mon, NNP), (amour, NNP), (oba...\n",
       "Name: normalized_tweet_2, dtype: object"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Die neue Normalisierungsfunktion auf Rohdaten (Tweets) anwenden\n",
    "train_data['normalized_tweet_2'] = train_data['tweet'].apply(normalize_data_2)\n",
    "test_data['normalized_tweet_2'] = test_data['tweet'].apply(normalize_data_2)\n",
    "dev_data['normalized_tweet_2'] = dev_data['tweet'].apply(normalize_data_2)\n",
    "\n",
    "train_data['normalized_tweet_2'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (['ga', 'hous', 'hit', 'go', 'chapel', 'hill', 'sat'], 'positive')\n",
      "1 (['theo', 'walcott', 'still', 'shit', 'watch', 'rafa', 'johnni', 'deal', 'saturday'], 'negative')\n",
      "2 (['gsp', 'fan', 'hate', 'nick', 'diaz', 'cant', 'wait', 'februari'], 'negative')\n",
      "3 (['iranian', 'gener', 'say', 'israel', 'iron', 'dome', 'cant', 'deal', 'missil', 'keep', 'talk', 'like', 'may', 'end', 'find'], 'negative')\n",
      "4 (['tehran', 'mon', 'amour', 'obama', 'tri', 'establish', 'tie', 'mullah', 'via', 'barack', 'obama', 'vote', 'mitt', 'romney'], 'neutral')\n",
      "5 (['sat', 'whole', 'movi', 'harri', 'ron', 'christma', 'ohlawd'], 'neutral')\n",
      "6 (['davlar', 'main', 'rival', 'team', 'poland', 'hope', 'make', 'success', 'end', 'tough', 'week', 'train', 'tomorrow'], 'positive')\n",
      "7 (['talk', 'act', 'sat', 'decid', 'want', 'colleg', 'appli', 'colleg', 'everyth', 'colleg', 'stress'], 'negative')\n",
      "8 (['whi', 'happi', 'valentin', 'trend', 'it', 'februari', 'june', 'smh'], 'neutral')\n",
      "9 (['they', 'may', 'superbowl', 'dalla', 'dalla', 'aint', 'win', 'superbowl', 'not', 'quarterback', 'owner'], 'negative')\n"
     ]
    }
   ],
   "source": [
    "# Entfernen der POS-Tags, da diese bei der späteren Evaluierung nicht mehr benötigt werden.\n",
    "tweets_train_2 = list(zip(train_data['normalized_tweet_2'].apply(remove_tags), train_data['sentiment']))    \n",
    "tweets_test_2 = list(zip(test_data['normalized_tweet_2'].apply(remove_tags), test_data['sentiment']))\n",
    "tweets_dev_2 = list(zip(dev_data['normalized_tweet_2'].apply(remove_tags), dev_data['sentiment']))\n",
    "\n",
    "for i, data in enumerate(tweets_train_2[:10]):\n",
    "    print(f'{i} {data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A4 und A5. Schritt 2: Adjektive und Adverbien aus den normalisierten Tweets extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19251\n"
     ]
    }
   ],
   "source": [
    "def get_all_adjectives_and_adverbs_from_tweets(tweets):\n",
    "    all_adjs_and_adverbs = []\n",
    "    for tweet in tweets:\n",
    "        for (word, tag) in tweet:\n",
    "            if tag.startswith('JJ') or tag.startswith('RB'):\n",
    "                all_adjs_and_adverbs.append(word)\n",
    "    \n",
    "    return all_adjs_and_adverbs\n",
    "\n",
    "all_adjs_and_adverbs_train = get_all_adjectives_and_adverbs_from_tweets(train_data['normalized_tweet_2'])\n",
    "all_adjs_and_adverbs_test = get_all_adjectives_and_adverbs_from_tweets(test_data['normalized_tweet_2'])\n",
    "all_adjs_and_adverbs_dev = get_all_adjectives_and_adverbs_from_tweets(dev_data['normalized_tweet_2'])\n",
    "all_adjs_and_adverbs = all_adjs_and_adverbs_train + all_adjs_and_adverbs_test + all_adjs_and_adverbs_dev\n",
    "print(len(all_adjs_and_adverbs))\n",
    "# print(nltk.FreqDist(all_adjs_and_adverbs).most_common(150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A4 und A5. Schritt 3: Die 1000 häufigsten Adjektive und Adverbien extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_2(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    most_common_features = wordlist.most_common(1000)\n",
    "    return [word for (word, freq) in most_common_features]\n",
    "\n",
    "word_features_2 = get_features_2(all_adjs_and_adverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A4 und A5. Schritt 4. Das Vorhandensein von den Adjektiven in den Tweets feststellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_2(tweet):\n",
    "    features = {}\n",
    "    for word in word_features_2:\n",
    "        features[f'contains {word}'] = (word in set(tweet))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Vorhandensein von den Adjektiven in den Tweets feststellen\n",
    "labeled_features_train_2 = [(extract_features_2(tweet), sentiment) for (tweet, sentiment) in tweets_train_2]\n",
    "labeled_features_test_2 = [(extract_features_2(tweet), sentiment) for (tweet, sentiment) in tweets_test_2]\n",
    "labeled_features_dev_2 = [(extract_features_2(tweet), sentiment) for (tweet, sentiment) in tweets_dev_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A4 und A5. Schritt 5: Naive Bayes Classifier trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_2 = NaiveBayesClassifier.train(labeled_features_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A6: Evaluierung durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes Classifier:  0.6543557936284183\n",
      "Most Informative Features\n",
      "          contains excit = True           positi : neutra =     39.6 : 1.0\n",
      "           contains fuck = True           negati : neutra =     36.9 : 1.0\n",
      "          contains happi = True           positi : neutra =     29.8 : 1.0\n",
      "          contains sorri = True           negati : neutra =     29.6 : 1.0\n",
      "           contains amaz = True           positi : neutra =     29.5 : 1.0\n",
      "         contains awesom = True           positi : neutra =     27.0 : 1.0\n",
      "          contains thank = True           positi : neutra =     25.1 : 1.0\n",
      "            contains sad = True           negati : neutra =     23.8 : 1.0\n",
      "           contains fail = True           negati : positi =     22.5 : 1.0\n",
      "          contains great = True           positi : neutra =     21.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy_2 = nltk.classify.accuracy(nb_classifier_2, labeled_features_test_2)\n",
    "print(\"Accuracy of Naive Bayes Classifier: \",accuracy_2)\n",
    "nb_classifier_2.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.376\n",
      "             2          -1.06387        0.404\n",
      "             3          -1.03565        0.404\n",
      "             4          -1.01413        0.404\n",
      "         Final          -0.99847        0.404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4299407950380603"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum Entropy Klassifikator trainiren und evaluieren\n",
    "maxent_classifier = MaxentClassifier.train(labeled_features_train_2, max_iter=5)\n",
    "nltk.classify.accuracy(maxent_classifier, labeled_features_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6478714406540739"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Klassifikator trainieren und evaluieren\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(labeled_features_train_2)\n",
    "nltk.classify.accuracy(SVC_classifier, labeled_features_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6433605864110515"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree Klassifikator trainieren und evaluieren\n",
    "decisiont_classifier = DecisionTreeClassifier.train(labeled_features_train_2)\n",
    "nltk.classify.accuracy(decisiont_classifier,  labeled_features_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A7: Zusammenfassung der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Rahmen des Projektes wurden folgende Klassifikationsmodelle trainiert: Naive Bayes, Maximum Entropy, Decision Tree sowie Support Vector Klassifikatoren. Die genannten Modelle analysieren die normalisierten Tweets, indem sie aufgrund von den definierten Features jeden der Tweets als neutral, positiv oder negativ einstufen. Die Accuracy-Werte der Modelle sehen wie folgt aus:\n",
    "\n",
    "- Die 1000 häufigsten Tokens aus den normalisierten Tweets als Features\n",
    "   - Der Accuracy-Wert von Naive Bayes Klassifikator: 0,639\n",
    "\n",
    "\n",
    "- Die 1000 häufigsten Adjektive und Adverbien aus den normalisierten Tweets als Features\n",
    "   - Der Accuracy-Wert von Naive Bayes Klassifikator: 0,654\n",
    "\n",
    "Man sieht, dass die 1000 häufigsten Adjektive und Adverbien als etwas bessere Features für das Modell erscheinen, da diese den Accuracy-Wert von Naive Bayes leicht erhöhen. Dies kann dafür sprechen, dass Adjektive und Adverbien mehr Informationsgehalt bezüglich der Stimmungswerte positiv, negativ und neutral beinhalten. Der Accuracy-Wert könnte wahrscheinlich noch erhöht werden, indem man einen besseren POS-Tagger implementiert sowie die Rechtsschreibkorrektur durchführt. Außerdem stellt sich noch die Frage, ob der Accuracy-Wert aufgrund von dem nicht balancierten Datensatz ein zuverlässiges Ergebnis ist und ob die Balancierung von Daten in Bezug auf die Stimmungswerte neutral, positiv und negativ nötig ist. \n",
    "\n",
    "Für das Trainieren der drei anderen Klassifikationsmodelle - Maximum Entropy, Support Vector und Decision Tree - wurden die 1000 häufigsten Adjektive und Adverbien als Features eingesetzt, da diese den Naive Bayes Klassifikator zu besseren Ergebnissen geführt haben. Die Accuracy-Werte der Modelle sehen jedoch etwas schlechter als die des Naive Bayes Klassifikators aus:\n",
    "\n",
    "- Die 1000 häufigsten Adjektive und Adverbien aus den normalisierten Tweets als Features\n",
    "  - Der Accuracy-Wert von Maximum Entropy Klassifikator: 0,429\n",
    "  - Der Accuracy-Wert von Support Vector Klassifikator: 0,647\n",
    "  - Der Accuracy-Wert von Decision Tree Klassifikator: 0,643\n",
    "\n",
    "Die schlechtesten Accuracy-Werte zeigt der Maximun Entropy Klassifikator. Die besseren Werte werden von den Klassifikatoren Support Vector und Decision Tree geliefert. Diese fallen jedoch etwas niedriger als die Werte von dem Naive Bayes Klassifikator aus. Trotz seiner naiven Grundannahme erzielt also der Naive Bayes Klassifikator die besten Ergebnisse im Rahmen des vorliegenden Projektes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
